from .common import *

class GLBPointCloudDensityFilter:
    """GLBç‚¹äº‘å¯†åº¦è¿‡æ»¤å™¨ - æ ¹æ®å±€éƒ¨å¯†åº¦åˆ é™¤ç¨€ç–åŒºåŸŸï¼Œä¿ç•™å¯†åº¦æœ€é«˜çš„æ ¸å¿ƒåŒºåŸŸ"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "glb_file_path": ("STRING", {
                    "default": "",
                    "tooltip": "GLBç‚¹äº‘æ–‡ä»¶è·¯å¾„ï¼šè¾“å…¥éœ€è¦è¿›è¡Œå¯†åº¦è¿‡æ»¤çš„GLBæ ¼å¼ç‚¹äº‘æ–‡ä»¶ã€‚æ”¯æŒç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºComfyUI outputç›®å½•çš„ç›¸å¯¹è·¯å¾„"
                }),
            },
            "optional": {
                "density_threshold": ("FLOAT", {
                    "default": 0.99, "min": 0.01, "max": 0.99, "step": 0.01,
                    "tooltip": "å¯†åº¦é˜ˆå€¼ï¼šä¿ç•™å¯†åº¦é«˜äºæ­¤æ¯”ä¾‹çš„ç‚¹ã€‚0.3=ä¿ç•™å¯†åº¦å‰70%çš„ç‚¹ï¼›0.5=ä¿ç•™å¯†åº¦å‰50%çš„ç‚¹ã€‚å€¼è¶Šå¤§åˆ é™¤è¶Šå¤šç¨€ç–ç‚¹"
                }),
                "neighborhood_radius": ("FLOAT", {
                    "default": 0.3, "min": 0.01, "max": 1.0, "step": 0.01,
                    "tooltip": "é‚»åŸŸåŠå¾„ï¼šè®¡ç®—å¯†åº¦æ—¶çš„æœç´¢åŠå¾„ã€‚è¾ƒå°å€¼=ç²¾ç»†å¯†åº¦æ£€æµ‹ï¼›è¾ƒå¤§å€¼=å¹³æ»‘å¯†åº¦æ£€æµ‹ã€‚å»ºè®®0.05-0.2"
                }),
                "min_neighbors": ("INT", {
                    "default": 5, "min": 1, "max": 50, "step": 1,
                    "tooltip": "æœ€å°é‚»å±…æ•°ï¼šä¸€ä¸ªç‚¹åœ¨é‚»åŸŸå†…è‡³å°‘éœ€è¦çš„é‚»å±…ç‚¹æ•°é‡æ‰ä¸è¢«è§†ä¸ºç¨€ç–ç‚¹ã€‚å€¼è¶Šå¤§è¦æ±‚å¯†åº¦è¶Šé«˜"
                }),
                "preserve_core_percentage": ("FLOAT", {
                    "default": 0.8, "min": 0.1, "max": 1.0, "step": 0.05,
                    "tooltip": "æ ¸å¿ƒåŒºåŸŸä¿ç•™æ¯”ä¾‹ï¼šæ— è®ºå¯†åº¦å¦‚ä½•ï¼Œéƒ½ä¼šä¿ç•™å¯†åº¦æœ€é«˜çš„è¿™éƒ¨åˆ†ç‚¹ã€‚0.8=ä¿è¯ä¿ç•™æœ€å¯†é›†çš„80%åŒºåŸŸ"
                }),
                "use_adaptive_radius": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "è‡ªé€‚åº”åŠå¾„ï¼šæ ¹æ®ç‚¹äº‘æ•´ä½“å°ºå¯¸è‡ªåŠ¨è°ƒæ•´é‚»åŸŸåŠå¾„ï¼Œç¡®ä¿åœ¨ä¸åŒå°ºå¯¸çš„ç‚¹äº‘ä¸Šéƒ½æœ‰è‰¯å¥½æ•ˆæœ"
                }),
                "output_filename": ("STRING", {
                    "default": "density_filtered_pointcloud",
                    "tooltip": "è¾“å‡ºGLBæ–‡ä»¶åï¼šå¯†åº¦è¿‡æ»¤åç‚¹äº‘çš„ä¿å­˜æ–‡ä»¶åï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ·»åŠ .glbæ‰©å±•å"
                }),
            }
        }

    RETURN_TYPES = (
        "STRING",    # å¤„ç†åçš„GLBæ–‡ä»¶è·¯å¾„
        "STRING",    # è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯JSON
    )
    RETURN_NAMES = (
        "filtered_glb_path",
        "filter_stats",
    )
    OUTPUT_TOOLTIPS = [
        "å¯†åº¦è¿‡æ»¤åçš„GLBæ–‡ä»¶å®Œæ•´è·¯å¾„",
        "è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯JSONï¼šåŒ…å«åŸå§‹ç‚¹æ•°ã€ä¿ç•™ç‚¹æ•°ã€åˆ é™¤ç‚¹æ•°ç­‰ç»Ÿè®¡æ•°æ®",
    ]
    OUTPUT_NODE = True
    FUNCTION = "filter_by_density"
    CATEGORY = "ğŸ’ƒVVL/Point Cloud Filtering"

    def filter_by_density(self,
                         glb_file_path: str,
                         density_threshold: float = 0.3,
                         neighborhood_radius: float = 0.1,
                         min_neighbors: int = 5,
                         preserve_core_percentage: float = 0.8,
                         use_adaptive_radius: bool = True,
                         output_filename: str = "density_filtered_pointcloud"):
        """
        æ ¹æ®å±€éƒ¨å¯†åº¦è¿‡æ»¤GLBç‚¹äº‘æ–‡ä»¶ï¼Œåˆ é™¤ç¨€ç–åŒºåŸŸ
        """
        
        processing_log = []
        processing_log.append("å¼€å§‹GLBç‚¹äº‘å¯†åº¦è¿‡æ»¤...")
        
        # æ£€æŸ¥ä¾èµ–
        if not TRIMESH_AVAILABLE:
            error_msg = "trimeshåº“ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†GLBæ–‡ä»¶"
            logger.error(error_msg)
            processing_log.append(f"é”™è¯¯: {error_msg}")
            return ("", "")
        
        # éªŒè¯è¾“å…¥æ–‡ä»¶è·¯å¾„
        if not glb_file_path or not glb_file_path.strip():
            error_msg = "GLBæ–‡ä»¶è·¯å¾„ä¸ºç©º"
            logger.error(error_msg)
            processing_log.append(f"é”™è¯¯: {error_msg}")
            return ("", "")
        
        # å¤„ç†æ–‡ä»¶è·¯å¾„
        input_path = self._resolve_file_path(glb_file_path.strip())
        processing_log.append(f"è¾“å…¥æ–‡ä»¶è·¯å¾„: {input_path}")
        
        if not os.path.exists(input_path):
            error_msg = f"GLBæ–‡ä»¶ä¸å­˜åœ¨: {input_path}"
            logger.error(error_msg)
            processing_log.append(f"é”™è¯¯: {error_msg}")
            return ("", "")
        
        try:
            # åŠ è½½GLBæ–‡ä»¶
            processing_log.append("æ­£åœ¨åŠ è½½GLBæ–‡ä»¶...")
            scene = trimesh.load(input_path)
            
            # æå–ç‚¹äº‘æ•°æ®
            point_clouds = []
            other_geometries = []
            
            if isinstance(scene, trimesh.Scene):
                for name, geometry in scene.geometry.items():
                    if isinstance(geometry, trimesh.PointCloud):
                        point_clouds.append((name, geometry))
                        processing_log.append(f"å‘ç°ç‚¹äº‘: {name}, ç‚¹æ•°: {len(geometry.vertices)}")
                    else:
                        other_geometries.append((name, geometry))
            elif isinstance(scene, trimesh.PointCloud):
                point_clouds.append(("main_pointcloud", scene))
                processing_log.append(f"å‘ç°ç‚¹äº‘: main_pointcloud, ç‚¹æ•°: {len(scene.vertices)}")
            else:
                # å°è¯•è½¬æ¢ä¸ºç‚¹äº‘
                if hasattr(scene, 'vertices'):
                    pc = trimesh.PointCloud(vertices=scene.vertices, colors=getattr(scene.visual, 'vertex_colors', None))
                    point_clouds.append(("converted_pointcloud", pc))
                    processing_log.append(f"è½¬æ¢ä¸ºç‚¹äº‘: converted_pointcloud, ç‚¹æ•°: {len(pc.vertices)}")
                else:
                    error_msg = "GLBæ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç‚¹äº‘æ•°æ®"
                    processing_log.append(f"é”™è¯¯: {error_msg}")
                    return ("", "")
            
            if not point_clouds:
                error_msg = "GLBæ–‡ä»¶ä¸­æ²¡æœ‰ç‚¹äº‘æ•°æ®"
                processing_log.append(f"é”™è¯¯: {error_msg}")
                return ("", "")
            
            # å¤„ç†æ¯ä¸ªç‚¹äº‘
            filtered_point_clouds = []
            total_original_points = 0
            total_filtered_points = 0
            
            for name, point_cloud in point_clouds:
                processing_log.append(f"\nå¤„ç†ç‚¹äº‘: {name}")
                original_count = len(point_cloud.vertices)
                total_original_points += original_count
                
                # è·å–åŸå§‹ç‚¹äº‘çš„æ‰€æœ‰å±æ€§å’ŒçŠ¶æ€
                original_vertices = point_cloud.vertices.copy()
                colors = None
                
                # è·å–é¢œè‰²ä¿¡æ¯
                if hasattr(point_cloud.visual, 'vertex_colors') and point_cloud.visual.vertex_colors is not None:
                    colors = point_cloud.visual.vertex_colors.copy()
                elif hasattr(point_cloud, 'colors') and point_cloud.colors is not None:
                    colors = point_cloud.colors.copy()
                
                # åº”ç”¨å¯†åº¦è¿‡æ»¤ï¼ˆä»…åŸºäºå¯†åº¦ï¼Œä¸æ”¹å˜é¡¶ç‚¹åæ ‡ï¼‰
                keep_mask, filter_info = self._apply_density_filter(
                    original_vertices, colors, density_threshold, neighborhood_radius, min_neighbors,
                    preserve_core_percentage, use_adaptive_radius, processing_log
                )
                
                # åº”ç”¨æ©ç ï¼Œä¿ç•™åŸå§‹çš„é¡¶ç‚¹åæ ‡
                filtered_vertices = original_vertices[keep_mask]
                filtered_colors = colors[keep_mask] if colors is not None else None
                
                filtered_count = len(filtered_vertices)
                total_filtered_points += filtered_count
                
                processing_log.append(f"ç‚¹äº‘ {name}: åŸå§‹ {original_count} -> ä¿ç•™ {filtered_count} (åˆ é™¤ {original_count - filtered_count})")
                
                # åˆ›å»ºè¿‡æ»¤åçš„ç‚¹äº‘ï¼Œä¿ç•™æ‰€æœ‰åŸå§‹å±æ€§
                if filtered_count > 0:
                    # ä½¿ç”¨åŸå§‹ç‚¹äº‘ä½œä¸ºæ¨¡æ¿ï¼Œåªæ›´æ–°é¡¶ç‚¹å’Œé¢œè‰²
                    filtered_pc = point_cloud.copy()
                    filtered_pc.vertices = filtered_vertices
                    if filtered_colors is not None:
                        filtered_pc.visual.vertex_colors = filtered_colors
                    
                    filtered_point_clouds.append((name, filtered_pc))
                    processing_log.append(f"ä¿ç•™ç‚¹äº‘ {name} çš„æ‰€æœ‰åŸå§‹å±æ€§å’ŒçŠ¶æ€")
                else:
                    processing_log.append(f"è­¦å‘Š: ç‚¹äº‘ {name} è¿‡æ»¤åæ²¡æœ‰å‰©ä½™ç‚¹")
            
            # ç›´æ¥ä¿®æ”¹åŸå§‹åœºæ™¯ï¼Œä¿æŒæ‰€æœ‰å‡ ä½•ä¿¡æ¯ä¸å˜
            if filtered_point_clouds:
                # ä½¿ç”¨åŸå§‹åœºæ™¯ä½œä¸ºåŸºç¡€ï¼Œç›´æ¥æ›¿æ¢ç‚¹äº‘æ•°æ®
                new_scene = scene
                
                # ç›´æ¥ä¿®æ”¹åŸå§‹åœºæ™¯ä¸­çš„ç‚¹äº‘å‡ ä½•ä½“ï¼Œä¿æŒå˜æ¢ä¸å˜
                for name, filtered_pc in filtered_point_clouds:
                    if isinstance(scene, trimesh.Scene):
                        # è·å–åŸå§‹å‡ ä½•ä½“
                        if name in scene.geometry:
                            original_geometry = scene.geometry[name]
                            
                            # å¦‚æœæ˜¯ç‚¹äº‘ï¼Œç›´æ¥ä¿®æ”¹é¡¶ç‚¹å’Œé¢œè‰²
                            if isinstance(original_geometry, trimesh.PointCloud):
                                original_geometry.vertices = filtered_pc.vertices
                                if hasattr(filtered_pc.visual, 'vertex_colors') and filtered_pc.visual.vertex_colors is not None:
                                    original_geometry.visual.vertex_colors = filtered_pc.visual.vertex_colors
                                
                                processing_log.append(f"ç›´æ¥ä¿®æ”¹åŸå§‹ç‚¹äº‘ {name}ï¼Œä¿æŒæ‰€æœ‰å˜æ¢å’Œå±æ€§ä¸å˜")
                            else:
                                # å¦‚æœä¸æ˜¯ç‚¹äº‘ç±»å‹ï¼Œæ›¿æ¢æ•´ä¸ªå‡ ä½•ä½“ä½†ä¿æŒå˜æ¢
                                scene.delete_geometry(name)
                                scene.add_geometry(filtered_pc, node_name=name)
                                processing_log.append(f"æ›¿æ¢å‡ ä½•ä½“ {name}ï¼Œå°è¯•ä¿æŒå˜æ¢")
                        else:
                            # æ–°å¢å‡ ä½•ä½“
                            scene.add_geometry(filtered_pc, node_name=name)
                            processing_log.append(f"æ·»åŠ æ–°ç‚¹äº‘ {name}")
                    else:
                        # å•ä¸ªå‡ ä½•ä½“çš„æƒ…å†µ
                        new_scene = filtered_pc
                        processing_log.append("å•ä¸ªå‡ ä½•ä½“ï¼Œç›´æ¥ä½¿ç”¨è¿‡æ»¤åçš„ç‚¹äº‘")
                
                processing_log.append("ç›´æ¥ä¿®æ”¹åŸå§‹åœºæ™¯ï¼Œæ‰€æœ‰å˜æ¢çŸ©é˜µå’Œå‡ ä½•å±æ€§å®Œå…¨ä¿æŒä¸å˜")
                
                # ç”Ÿæˆè¾“å‡ºè·¯å¾„
                output_path = self._generate_output_path(output_filename)
                processing_log.append(f"è¾“å‡ºæ–‡ä»¶è·¯å¾„: {output_path}")
                
                # ä¿å­˜è¿‡æ»¤åçš„GLBæ–‡ä»¶
                processing_log.append("æ­£åœ¨ä¿å­˜è¿‡æ»¤åçš„GLBæ–‡ä»¶...")
                new_scene.export(output_path)
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    processing_log.append(f"GLBæ–‡ä»¶ä¿å­˜æˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {file_size} bytes")
                    
                    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
                    filter_stats = {
                        "original_points": total_original_points,
                        "filtered_points": total_filtered_points,
                        "removed_points": total_original_points - total_filtered_points,
                        "retention_rate": total_filtered_points / total_original_points if total_original_points > 0 else 0,
                        "parameters": {
                            "density_threshold": density_threshold,
                            "neighborhood_radius": neighborhood_radius,
                            "min_neighbors": min_neighbors,
                            "preserve_core_percentage": preserve_core_percentage,
                            "use_adaptive_radius": use_adaptive_radius
                        }
                    }
                    
                    stats_json = json.dumps(filter_stats, indent=2)
                    
                    processing_log.append("GLBç‚¹äº‘å¯†åº¦è¿‡æ»¤å®Œæˆ! å·²ä¿ç•™åŸå§‹æ¨¡å‹çš„å¤§å°ã€æœå‘å’Œå‡ ä½•å±æ€§")
                    processing_log.append(f"ç»Ÿè®¡: åŸå§‹{total_original_points}ç‚¹ -> ä¿ç•™{total_filtered_points}ç‚¹ (ä¿ç•™ç‡: {filter_stats['retention_rate']:.1%})")
                    
                    return (output_path, stats_json)
                else:
                    error_msg = "GLBæ–‡ä»¶ä¿å­˜å¤±è´¥"
                    processing_log.append(f"é”™è¯¯: {error_msg}")
                    return ("", "")
            else:
                error_msg = "è¿‡æ»¤åæ²¡æœ‰å‰©ä½™çš„ç‚¹äº‘æ•°æ®"
                processing_log.append(f"é”™è¯¯: {error_msg}")
                return ("", "")
                
        except Exception as e:
            error_msg = f"å¤„ç†GLBæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(error_msg)
            processing_log.append(f"é”™è¯¯: {error_msg}")
            import traceback
            traceback.print_exc()
            return ("", "")
    
    def _apply_density_filter(self, vertices, colors, density_threshold, neighborhood_radius, 
                             min_neighbors, preserve_core_percentage, use_adaptive_radius, processing_log):
        """
        åº”ç”¨å¯†åº¦è¿‡æ»¤ç®—æ³•ï¼Œè¿”å›ä¿ç•™ç‚¹çš„æ©ç 
        """
        
        n_points = len(vertices)
        processing_log.append(f"å¼€å§‹å¯†åº¦åˆ†æï¼Œæ€»ç‚¹æ•°: {n_points}")
        
        # è‡ªé€‚åº”åŠå¾„è°ƒæ•´
        if use_adaptive_radius:
            # è®¡ç®—ç‚¹äº‘çš„åŒ…å›´ç›’å¯¹è§’çº¿é•¿åº¦
            bbox_min = np.min(vertices, axis=0)
            bbox_max = np.max(vertices, axis=0)
            bbox_diagonal = np.linalg.norm(bbox_max - bbox_min)
            
            # æ ¹æ®åŒ…å›´ç›’å¤§å°è°ƒæ•´åŠå¾„
            adaptive_radius = neighborhood_radius * bbox_diagonal / 10.0  # å¯è°ƒæ•´çš„æ¯”ä¾‹å› å­
            processing_log.append(f"è‡ªé€‚åº”åŠå¾„: {neighborhood_radius} -> {adaptive_radius:.4f} (åŸºäºåŒ…å›´ç›’å¯¹è§’çº¿ {bbox_diagonal:.4f})")
            neighborhood_radius = adaptive_radius
        
        # ä½¿ç”¨KDTreeè¿›è¡Œé«˜æ•ˆçš„é‚»åŸŸæœç´¢
        try:
            from scipy.spatial import cKDTree
            kdtree = cKDTree(vertices)
            processing_log.append("ä½¿ç”¨scipy.spatial.cKDTreeè¿›è¡Œé‚»åŸŸæœç´¢")
        except ImportError:
            processing_log.append("scipyä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„è·ç¦»è®¡ç®—")
            kdtree = None
        
        # è®¡ç®—æ¯ä¸ªç‚¹çš„å±€éƒ¨å¯†åº¦
        densities = np.zeros(n_points)
        
        BIG_POINT_THRESHOLD = 200000  # 20ä¸‡ç‚¹ä»¥ä¸Šä½¿ç”¨ä½“ç´ ç»Ÿè®¡
        if n_points > BIG_POINT_THRESHOLD:
            # ------------------------------------------------------------------
            # å¤§è§„æ¨¡ç‚¹äº‘ï¼šä½¿ç”¨ä½“ç´ ç»Ÿè®¡è¿‘ä¼¼å¯†åº¦ (O(N) å†…å­˜, éé€’å½’, æ— KDTree)
            # ------------------------------------------------------------------
            start_t = time.time()
            bbox_min = vertices.min(axis=0)
            voxel_size = neighborhood_radius  # ä½“ç´ è¾¹é•¿ä¸é‚»åŸŸåŠå¾„ä¸€è‡´
            voxel_indices = np.floor((vertices - bbox_min) / voxel_size).astype(np.int32)
            # ä½¿ç”¨ç»“æ„åŒ–dtypeä¾¿äºunique
            voxel_keys = voxel_indices.view([('x', np.int32), ('y', np.int32), ('z', np.int32)]).reshape(-1)
            unique_keys, inverse_indices, counts = np.unique(voxel_keys, return_inverse=True, return_counts=True)
            densities = counts[inverse_indices] - 1  # åŒä½“ç´ å†…ç‚¹æ•°è¿‘ä¼¼é‚»å±…æ•°
            elapsed = time.time() - start_t
            processing_log.append(f"ä½“ç´ å¯†åº¦è®¡ç®—å®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f}s (ä½“ç´ æ•°={len(unique_keys):,})")
        elif kdtree is not None:
            # -----------------------------
            # ä½¿ç”¨KDTreeæ‰¹é‡å¹¶è¡ŒæŸ¥è¯¢æé«˜é€Ÿåº¦
            # -----------------------------
            start_t = time.time()
            neighbors_list = kdtree.query_ball_point(vertices, neighborhood_radius, workers=-1)
            densities = np.fromiter((len(idx) - 1 for idx in neighbors_list), dtype=np.int32, count=n_points)
            elapsed = time.time() - start_t
            processing_log.append(f"KDTreeå¯†åº¦è®¡ç®—å®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f}s (å¹¶è¡ŒæŸ¥è¯¢)")
        else:
            # -----------------------------
            # å›é€€åˆ°ç®€åŒ–çš„è·ç¦»è®¡ç®—ï¼ˆä»…åœ¨å°ç‚¹äº‘æˆ–æ— SciPyæ—¶ä½¿ç”¨ï¼‰
            # -----------------------------
            start_t = time.time()
            # é‡‡ç”¨å‘é‡åŒ–å¹¿æ’­ä¸€æ¬¡æ€§è®¡ç®—è·ç¦»çŸ©é˜µï¼ˆO(N^2) å†…å­˜æ¶ˆè€—å¤§ï¼Œä»…é™<50kç‚¹ï¼‰
            if n_points < 50000:
                dist_matrix = np.linalg.norm(vertices[None, :, :] - vertices[:, None, :], axis=2)
                densities = (dist_matrix <= neighborhood_radius).sum(axis=1) - 1  # æ’é™¤è‡ªèº«
            else:
                # å¤§ç‚¹äº‘æ—¶é€€åŒ–ä¸ºåˆ†æ‰¹å¾ªç¯
                for i in range(n_points):
                    distances = np.linalg.norm(vertices - vertices[i], axis=1)
                    neighbor_count = np.sum(distances <= neighborhood_radius) - 1
                    densities[i] = neighbor_count
            elapsed = time.time() - start_t
            processing_log.append(f"å‘é‡åŒ–å¯†åº¦è®¡ç®—å®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f}s")
        
        processing_log.append(f"å¯†åº¦ç»Ÿè®¡: æœ€å°={densities.min():.1f}, æœ€å¤§={densities.max():.1f}, å¹³å‡={densities.mean():.1f}")
        
        # åº”ç”¨å¤šå±‚è¿‡æ»¤ç­–ç•¥
        keep_mask = np.ones(n_points, dtype=bool)
        
        # 1. åŸºäºæœ€å°é‚»å±…æ•°çš„è¿‡æ»¤
        min_neighbor_mask = densities >= min_neighbors
        removed_by_min_neighbors = np.sum(~min_neighbor_mask)
        keep_mask &= min_neighbor_mask
        processing_log.append(f"æœ€å°é‚»å±…æ•°è¿‡æ»¤: åˆ é™¤ {removed_by_min_neighbors} ä¸ªç‚¹")
        
        # 2. åŸºäºå¯†åº¦é˜ˆå€¼çš„è¿‡æ»¤
        if density_threshold > 0 and np.sum(keep_mask) > 0:
            valid_densities = densities[keep_mask]
            density_percentile = np.percentile(valid_densities, (1 - density_threshold) * 100)
            density_mask = densities >= density_percentile
            
            # ä¿ç•™å½“å‰maskä¸­çš„ç‚¹ï¼Œå†åº”ç”¨å¯†åº¦è¿‡æ»¤
            temp_mask = keep_mask.copy()
            keep_mask[temp_mask] &= density_mask[temp_mask]
            
            removed_by_density = np.sum(temp_mask) - np.sum(keep_mask)
            processing_log.append(f"å¯†åº¦é˜ˆå€¼è¿‡æ»¤: åˆ é™¤ {removed_by_density} ä¸ªç‚¹ (é˜ˆå€¼å¯†åº¦: {density_percentile:.1f})")
        
        # 3. æ ¸å¿ƒåŒºåŸŸä¿æŠ¤
        if preserve_core_percentage < 1.0 and np.sum(keep_mask) > 0:
            # è®¡ç®—éœ€è¦ä¿ç•™çš„æ ¸å¿ƒç‚¹æ•°
            target_core_points = int(n_points * preserve_core_percentage)
            current_points = np.sum(keep_mask)
            
            if current_points < target_core_points:
                # å½“å‰ä¿ç•™çš„ç‚¹æ•°å°‘äºæ ¸å¿ƒè¦æ±‚ï¼Œéœ€è¦æ·»åŠ é«˜å¯†åº¦ç‚¹
                need_more_points = target_core_points - current_points
                
                # æ‰¾åˆ°å¯†åº¦æœ€é«˜çš„ç‚¹
                excluded_indices = np.where(~keep_mask)[0]
                if len(excluded_indices) > 0:
                    excluded_densities = densities[excluded_indices]
                    # æŒ‰å¯†åº¦æ’åºï¼Œé€‰æ‹©å¯†åº¦æœ€é«˜çš„ç‚¹
                    sorted_indices = excluded_indices[np.argsort(-excluded_densities)]
                    points_to_restore = min(need_more_points, len(sorted_indices))
                    
                    keep_mask[sorted_indices[:points_to_restore]] = True
                    processing_log.append(f"æ ¸å¿ƒåŒºåŸŸä¿æŠ¤: æ¢å¤ {points_to_restore} ä¸ªé«˜å¯†åº¦ç‚¹")
        
        # ç»Ÿè®¡ä¿¡æ¯
        original_count = n_points
        filtered_count = np.sum(keep_mask)
        removed_count = original_count - filtered_count
        
        filter_info = {
            "original_points": original_count,
            "filtered_points": filtered_count,
            "removed_points": removed_count,
            "retention_rate": filtered_count / original_count if original_count > 0 else 0,
            "density_stats": {
                "min": float(densities.min()),
                "max": float(densities.max()),
                "mean": float(densities.mean()),
                "std": float(densities.std())
            }
        }
        
        processing_log.append(f"å¯†åº¦è¿‡æ»¤å®Œæˆ: ä¿ç•™ {filtered_count}/{original_count} ç‚¹ (ä¿ç•™ç‡: {filter_info['retention_rate']:.1%})")
        
        return keep_mask, filter_info
    
    def _resolve_file_path(self, file_path: str) -> str:
        """è§£ææ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„"""
        if os.path.isabs(file_path):
            return file_path
        
        # å°è¯•ç›¸å¯¹äºComfyUIè¾“å‡ºç›®å½•
        if FOLDER_PATHS_AVAILABLE:
            output_dir = folder_paths.get_output_directory()
            candidate_path = os.path.join(output_dir, file_path)
            if os.path.exists(candidate_path):
                return candidate_path
        
        # å°è¯•ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
        if os.path.exists(file_path):
            return os.path.abspath(file_path)
        
        # è¿”å›åŸå§‹è·¯å¾„ï¼ˆè®©åç»­æ£€æŸ¥å¤„ç†é”™è¯¯ï¼‰
        return file_path
    
    def _generate_output_path(self, filename: str) -> str:
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„"""
        if FOLDER_PATHS_AVAILABLE:
            output_dir = folder_paths.get_output_directory()
        else:
            output_dir = "output"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # å¤„ç†æ–‡ä»¶åï¼Œç¡®ä¿æ˜¯.glbæ‰©å±•å
        if not filename.lower().endswith('.glb'):
            filename = f"{filename}.glb"
        
        return os.path.join(output_dir, filename)


